import pandas as pd
import numpy as np
from collections import defaultdict, Counter
import re
import logging
from datetime import datetime
# Removed rapidfuzz import - not needed for exact matching only
import streamlit as st
import time




class AddressNormalizer:
    def __init__(self):
        self.aliases = {
            "‘Ω. ’Ä‘±’Ö’ê‘ª‘ø": "‘Ω’ê‘ª’Ñ’Ö‘±’Ü ’Ä‘±’Ö’ê‘ª‘ø‘ª",
            "‘Ω’ê‘ª’Ñ’Ö‘±’Ü ’Ä‘±’Ö’ê‘ª‘ø": "‘Ω’ê‘ª’Ñ’Ö‘±’Ü ’Ä‘±’Ö’ê‘ª‘ø‘ª",
        }

        self.armenian_suffixes = [
            r'\b‘Ω’É’Ç\.?',
            r'\b‘Ω’É\.?',
            r'\b÷Ä’§\.?',
            r'\b’´’∂\.?',
            r'\b’Ü’ê‘≤\.?',
            r'\b’∂÷Ä’¢\.?',
            r'\b’Ü÷Ä’¢\.?',
            r'\b’á‘±’ê’î\.?',
            r'\b’á’°÷Ä÷Ñ\.?',
            r'\b’∑’°÷Ä÷Ñ\.?',
            r'\‘±’∂÷Å’≤\.?',
            r'\b‘±’Ü’ë’Ç\.?',
            r'\b’ì’Ø’≤\.?',
            r'\b’ì‘ø’Ç\.?',
            r'\b‘±’Ü’ë’Ç\.?',
            r'\b’∑’°÷Ä÷Ñ\.?',
            r'\b‘≤’Ü\.?',
            r'\b’É’Ç\.?',
            r'\b’ì\.?',
            r'\b’ä’à’Ç\.?',
            r'\b’ä\.?',
            r'\b‘±’é\.?',
            r'\b’É‘±’Ñ‘≤\.?',
            r'\b’è’à’í’Ü\.?',
            r'\b’è‚Ä§\.?',
            r'\b\.?',
            r'\b’Ä’∏’≤\.?',
            r'\b’ì‘±‘ø\.?',
            r'\b’∑’°÷Ä÷Ñ\.?',
            r'\b’©’≤’¥\.?',
            r'\b‘π’Ç’Ñ\.?',
            r'\b’©’°’≤’°’¥’°’Ω’´\.?',
            r'\b‘π‘µ‘º‘±\.?',
            r'\b’É‘±’Ü\.?',
            r'\b’á÷Ä’ª\.?',
            r'\b’©/’≤\.?',
            r'\’©’≤’¥',
        ]

        # Normalize old and new keys in this mapping
        self.old_to_new_map = {
            self._norm("’ñ÷Ä’∏÷Ç’∂’¶’•’´"): self._norm("‘º. ’Ñ’°’§’∏’µ’°’∂"),
            self._norm("‘º’•’∂’´’∂’£÷Ä’°’§’µ’°’∂"): self._norm("’é. ’ç’°÷Ä’£’Ω’µ’°’∂"),
            self._norm("‘ø’´÷Ä’∏’æ’°’Ø’°’∂’µ’°’∂"): self._norm("’é’°’∂’°’±’∏÷Ä’´"),
            self._norm("‘ø’°’¨’´’∂’´’∂’´"): self._norm("‘≥. ’Ü’™’§’•’∞’´"),
            self._norm("‘ø’´’∂’£’´’Ω’•’∫’´"): self._norm("’é. ’â’•÷Ä’°’¶’´"),
            self._norm("’ä’¨’•’≠’°’∂’∏’æ’´"): self._norm("’ç’°’∞’¥’°’∂’°’∫’°’∞’∂’•÷Ä’´"),
            self._norm("’á’´’∂’°÷Ä’°÷Ä’∂’•÷Ä’´"): self._norm("’Ñ. ‘π’•’ø’π’•÷Ä’´"),
            self._norm("‘ø’´÷Ä’∏’æ’´"): self._norm("’Ü. ’å’´’™’Ø’∏’æ’´"),
            self._norm("‘º’•’∂’´’∂’´"): self._norm("’è’´’£÷Ä’°’∂ ’Ñ’•’Æ’´"),
            self._norm("‘Ω. ’Ä’°’µ÷Ä’´’Ø"): self._norm("‘Ω÷Ä’´’¥’µ’°’∂ ’Ä’°’µ÷Ä’´’Ø’´"),
            self._norm("‘±’∂’´ ’©’°’≤’°’¥’°’Ω ’Ñ. ‘±’æ’•’ø’´’Ω’µ’°’∂"): self._norm("’Ñ. ‘±’æ’•’ø’´’Ω’µ’°’∂"),
            self._norm("’Ñ’°÷Ä÷Ñ’Ω’´"): self._norm("’ä. ’ã’°÷É’°÷Ä’´’±’•’´"),
            self._norm("‘±’∂’´ ’©’°’≤’°’¥’°’Ω ‘±. ’á’°’∞’´’∂’µ’°’∂"): self._norm("‘±. ’á’°’∞’´’∂’µ’°’∂"),
            self._norm("’ï’≤’°’Ø’°’µ’´’∂"): self._norm("‘±÷Ä÷á’•’¨’µ’°’∂ ’∑÷Ä’ª’°’∂÷Å’∏’≤"),
            self._norm("’å’•’∫’´’∂’´"): self._norm("‘≤. ’á’π’•÷Ä’¢’´’∂’°’µ’´"),
            self._norm("’Ä’•’≤’°÷É’∏’≠’∏÷Ç’©’µ’°’∂"): self._norm("‘≥. ’Ü’™’§’•’∞’´"),
            self._norm("‘±’∂’´ ’©’°’≤’°’¥’°’Ω ‘µ. ’â’°÷Ä’•’∂÷Å’´"): self._norm("‘µ. ’â’°÷Ä’•’∂÷Å’´"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 10-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 15-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 11-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 12-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 13-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 14-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’ç÷á’µ’°’∂"): self._norm("’Ä. ’Ç’°’∂’§’´’¨’µ’°’∂"),
            self._norm("’Ñ’∏÷Ç’∑-2  ’©’°’≤’°’¥’°’Ω’´ ÷É’∏’≤’∏÷Å’∂’•÷Ä’´÷Å ’¥’•’Ø’®"): self._norm("‘ø. ’Ä’°’¨’°’¢’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("‘Ω’°’≤’°’≤’∏÷Ç’©’µ’°’∂"): self._norm("‘≤’°’£÷Ä’°’ø’∏÷Ç’∂’µ’°÷Å"),
            self._norm("’Ñ’°÷Ä÷Ñ’Ω’´"): self._norm("’ã’´’æ’°’∂’∏÷Ç"),
            self._norm("‘±’¶’´’¶’¢’•’Ø’∏’æ’´"): self._norm("’Ü. ’á’∂’∏÷Ä’∞’°’¨’∏÷Ç"),
            self._norm("‘∑’¨’•’Ø’ø÷Ä’∏ ’∫÷Ä’´’¢’∏÷Ä’∂’´ 6-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘±. ‘±÷Ä’¥’•’∂’µ’°’∂"),
            self._norm("‘∑’¨’•’Ø’ø÷Ä’∏ ’∫÷Ä’´’¢’∏÷Ä’∂’´ 10-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘±. ‘≥÷á’∏÷Ä’£’µ’°’∂"),
            self._norm("‘ø’´÷Ä’∏’æ’°’¢’°’§’µ’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("‘±. ‘π’°’¥’°’∂’µ’°’∂"),
            self._norm("50 ’°’¥’µ’°’Ø’´ ’°’∂’æ’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("‘±. ’Ñ’°’∂’∏÷Ç’Ø’µ’°’∂"),
            self._norm("<<‘±’∂’´>> ’©’°’≤’°’¥’°’Ω 3-÷Ä’§ ÷É’∏’≤’∏÷Å"): self._norm("‘±. ’á’°’∞’´’∂’µ’°’∂"),
            self._norm("’Ä’∂’∏÷Å’°’æ’°’∂ 2-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘±. ’ä’•’ø÷Ä’∏’Ω’µ’°’∂"),
            self._norm("‘ø’∏’¥’Ω’∏’¥’∏’¨’´ ÷É’∏’≤’∏÷Å"): self._norm("‘±. ’é’°’Ω’´’¨’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘ø’•÷Å’≠’∏’æ’•’¨’´ ÷É’∏’≤’∏÷Å"): self._norm("‘±÷Ä’ø’°’Ø ’•’∫’´’Ω’Ø’∏’∫’∏’Ω ’ç’¥’¢’°’ø’µ’°’∂"),
            self._norm("‘±÷Ä’æ’•’¨’°’±’• ÷É’∏’≤’∏÷Å"): self._norm("‘≥’°÷Ä’•’£’´’∂ ‘±"),
            self._norm("‘∑’¨’•’Ø’ø÷Ä’∏ ’∫÷Ä’´’¢’∏÷Ä’∂’´ 8-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘π. ’Ñ’°’∂’§’°’¨’µ’°’∂"),
            self._norm("’ä’∏’≤’∫’°’ø’°’æ’°’∂ 3-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘∫. ‘≤. ‘≤’°÷Ä’∏’∂’µ’°’∂"),
            self._norm("‘ø÷Ä’∏÷Ç’∫’Ω’Ø’°’µ’° ÷É’∏’≤’∏÷Å"): self._norm("‘Ω. ‘¥’°’∑’ø’•’∂÷Å’´"),
            self._norm("’î’∏÷Ç’©’°’´’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("‘ø. ‘¥’•’¥’´÷Ä’≥’µ’°’∂"),
            self._norm("’ä’∏’≤’∫’°’ø’°’æ’°’∂ 2-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘ø. ‘Ω’°’π’°’ø÷Ä’µ’°’∂"),
            self._norm("‘ø’∏÷Ç’µ’¢’´’∑÷á’´ ÷É’∏’≤’∏÷Å"): self._norm("’Ä. ’Ñ’°’¶’¥’°’∂’µ’°’∂"),
            self._norm("’ä’´’∏’∂’•÷Ä’°’Ø’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("’Ä. ’Ñ’•’¨÷Ñ’∏’∂’µ’°’∂"),
            self._norm("’ä’∏’≤’∫’°’ø’°’æ’°’∂ 1-’´’∂ ’∑’°÷Ä÷Ñ"): self._norm("’Ä. ’ä’∏’≤’∏’Ω’µ’°’∂"),
            self._norm("’ä’∏’≤’∫’°’ø’°’æ’°’∂ 4-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("’Ä. ’å’°’Ω’Ø’°’ø’¨’µ’°’∂"),
            self._norm("‘ø’°’ø’•’¨’∂’°’µ’°"): self._norm("’Ä’∂’∏÷Å’°’æ’°’∂’´ 1-’´’∂ ’∑’°÷Ä÷Ñ"),
            self._norm("’ä’•’ø ’¢’°÷Ä’°’Ø’∂’•÷Ä"): self._norm("’Ç. ’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("’Ñ’°’µ’´’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("’Ñ. ’Ñ’Ø÷Ä’ø’π’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘∑’¨’•’Ø’ø÷Ä’∏ ’∫÷Ä’´’¢’∏÷Ä’∂’´ 7-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("’Ñ. ’ç’°÷Ä’£’Ω’µ’°’∂"),
            self._norm("’ç’æ’•÷Ä’§’¨’∏’æ’´ ÷É’∏’≤’∏÷Å"): self._norm("’Ü. ’Ç’∏÷Ä’≤’°’∂’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘±’Ω’ø’≤’´ ’∞÷Ä’°’∫’°÷Ä’°’Ø"): self._norm("’á. ‘±’¶’∂’°’æ’∏÷Ç÷Ä"),
            self._norm("’ç. ’Ñ’∏÷Ç’Ω’°’µ’•’¨’µ’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("’á. ‘±’¶’∂’°’æ’∏÷Ç÷Ä"),
            self._norm("‘∑’¨’•’Ø’ø÷Ä’∏ ’∫÷Ä’´’¢’∏÷Ä’∂’´ 11-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("’å. ‘¥’°’∂’´’•’¨’µ’°’∂"),
            self._norm("’ï÷Ä’ª’∏’∂’´’Ø’´’±’•’´ ÷É’∏’≤’∏÷Å"): self._norm("’ç. ’Ñ’°’ø’∂’´’∑’µ’°’∂"),
            self._norm("‘∑’∂’£’•’¨’Ω’´ ÷É’∏’≤’∏÷Å"): self._norm("’é. ‘±’≥’•’¥’µ’°’∂"),
            self._norm("‘ø’•’∂’ø÷Ä’∏’∂’°’Ø’°’∂ ’∞÷Ä’°’∫’°÷Ä’°’Ø"): self._norm("’é’°÷Ä’§’°’∂’°’∂÷Å"),
            self._norm("<<‘±’∂’´>> ’©’°’≤’°’¥’°’Ω 15-÷Ä’§ ÷É’∏’≤’∏÷Å"): self._norm("’ñ’∏÷Ä’°’¨’¢’•÷Ä’£’´"),
        }

    def _norm(self, text):
        text = str(text).strip()

        # Normalize Armenian "and" BEFORE upper() to handle both cases
        # Convert all forms to the ligature ÷á for standardization
        text = text.replace('‘µ’é', '÷á')  # uppercase separate chars ‚Üí ligature
        text = text.replace('’•’æ', '÷á')  # lowercase separate chars ‚Üí ligature
        text = text.replace('‘±’∂’ø’°’º’°’æ’°’∂', '‘±’∂’ø’°’º')  # lowercase separate chars ‚Üí ligature
        text = text.replace('‘±’Ü’è‘±’å‘±’é‘±’Ü', '‘±’Ü’è‘±’å')  # lowercase separate chars ‚Üí ligature
        text = text.replace('’Ä’ê‚Ä§', '’Ä’ê‘±’ä‘±’ê‘±‘ø')  # lowercase separate chars ‚Üí ligature
        text = text.replace('’Ñ’à’í’á÷ä2', '’Ñ’à’í’á 2')  # lowercase separate chars ‚Üí ligature

        # Now convert to uppercase (which will preserve the ÷á ligature)
        text = text.upper()
        
        # Normalize Armenian ordinal numbers to Arabic numerals
        # Patterns: 1-’´’∂, 2-÷Ä’§, 3-÷Ä’§, etc. (handles various dash types)
        text = re.sub(r'(\d+)[-÷ä‚Äê‚Äë‚Äí‚Äì‚Äî‚Äï]?(’ê‘¥|’Ü‘¥|‘ª’Ü|‘±’Ñ)\.?', r'\1', text, flags=re.IGNORECASE)
        
        # Remove word ’Ñ‘±’ê’á‘±‘º (case-insensitive)
        text = re.sub(r'\b’Ñ‘±’ê’á‘±‘º\b', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\b’á‘±’ê‘º\b', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\b’á‘±’å‘º\b', '', text, flags=re.IGNORECASE)

        # Remove one-letter Armenian words (with or without dot, case-insensitive)
        text = re.sub(r'(?<!\S)[‘±-’ñ]\.?(?!\S)', '', text, flags=re.IGNORECASE)
        
        # Remove suffixes
        for suffix in self.armenian_suffixes:
            text = re.sub(suffix, '', text, flags=re.IGNORECASE)
        
        # Strip unwanted punctuation (preserve / and -)
        text = re.sub(r'[^\w\s/\-]', '', text)
        
        # Remove trailing ‘ª if present
        text = ' '.join([w[:-1] if w.endswith("‘ª") else w for w in text.split()])
        text = ' '.join([w[:-2] if w.endswith("’Ö‘ª") else w for w in text.split()])
        text = ' '.join([w[:-2] if w.endswith("’à’í") else w for w in text.split()])

        return re.sub(r'\s+', ' ', text).strip()

    def normalize_number_part(self, text):
        if pd.isna(text):
            return ""
        text = str(text).strip().upper()
        
        # Normalize Armenian ordinal numbers in house/building numbers
        # Patterns: 1-’´’∂, 2-÷Ä’§, 3-÷Ä’§, etc. (handles various dash types)
        text = re.sub(r'(\d+)[-÷ä‚Äê‚Äë‚Äí‚Äì‚Äî‚Äï]?(’ê‘¥|’Ü‘¥|‘ª’Ü|‘±’Ñ)\.?', r'\1', text, flags=re.IGNORECASE)
        
        return re.sub(r'[^\w/\-]', '', text)

    def normalize(self, text):
        if pd.isna(text):
            return ""

        text = str(text).strip().upper()

        # Apply direct alias replacement first
        if text in self.aliases:
            text = self.aliases[text]

        # Apply normalization
        text_norm = self._norm(text)
        
        # Apply final mapping
        return self.old_to_new_map.get(text_norm, text_norm)


class AdvancedAddressMatcher:
    def __init__(self, spr_df, cad_df, max_records=None):
        # Limit SPR records if specified
        if max_records is not None and max_records < len(spr_df):
            self.spr_df = spr_df.head(max_records).copy()
            st.info(f"üìä Processing limited to {max_records:,} SPR records out of {len(spr_df):,} total records")
        else:
            self.spr_df = spr_df.copy()

        self.cad_df = cad_df
        self.original_spr_count = len(spr_df)
        self.processing_count = len(self.spr_df)

        # Create indices based on the limited dataset
        self.street_index = self._create_street_index()
        self.house_index = self._create_house_index()

    def _create_street_index(self):
        """Create street-based index for fast lookups"""
        index = defaultdict(list)
        for idx, row in self.cad_df.iterrows():
            street = row['STREET_NORM']
            if street:
                index[street].append(idx)
        return dict(index)

    def _create_house_index(self):
        """Create house-based index for fast lookups"""
        index = defaultdict(list)
        for idx, row in self.cad_df.iterrows():
            house = row['HOUSE_NORM']
            if house:
                index[house].append(idx)
        return dict(index)


    def find_exact_matches(self):
        """Find exact matches using full address matching with detailed progress"""
        matches = []
        total_spr = len(self.spr_df)

        # Create progress tracking components
        progress_container = st.container()
        with progress_container:
            st.subheader("üéØ Exact Matching Progress")
            progress_bar = st.progress(0)
            status_text = st.empty()
            stats_cols = st.columns(4)

            with stats_cols[0]:
                processed_metric = st.empty()
            with stats_cols[1]:
                matches_metric = st.empty()
            with stats_cols[2]:
                rate_metric = st.empty()
            with stats_cols[3]:
                eta_metric = st.empty()

        start_time = time.time()

        # Full address exact match
        status_text.text("üîç Finding exact address matches...")
        cad_full_lookup = {row['FULL_ADDRESS']: idx for idx, row in self.cad_df.iterrows()}

        chunk_size = 500

        for i in range(0, total_spr, chunk_size):
            if st.session_state.get('stop_requested', False):
                break

            chunk = self.spr_df.iloc[i:i + chunk_size]
            chunk_matches = 0

            for idx, spr_row in chunk.iterrows():
                full_address = spr_row['FULL_ADDRESS']
                if full_address in cad_full_lookup:
                    cad_idx = cad_full_lookup[full_address]
                    matches.append(self._create_match_record(spr_row, self.cad_df.iloc[cad_idx], 100, "EXACT_FULL"))
                    chunk_matches += 1

            # Update progress
            processed = min(i + chunk_size, total_spr)
            progress = processed / total_spr
            elapsed_time = time.time() - start_time

            # Calculate ETA
            if processed > 0:
                rate = processed / elapsed_time
                eta = (total_spr - processed) / rate if rate > 0 else 0
            else:
                eta = 0

            # Update UI
            progress_bar.progress(progress)
            status_text.text(
                f"Processed {processed:,}/{total_spr:,} records | Found {chunk_matches} matches in current chunk")

            processed_metric.metric("Processed", f"{processed:,}/{total_spr:,}")
            matches_metric.metric("Exact Matches", f"{len(matches):,}")
            rate_metric.metric("Rate", f"{rate:.1f} records/sec" if rate > 0 else "Calculating...")
            eta_metric.metric("ETA", f"{eta:.1f}s" if eta > 0 else "Calculating...")

        # Final update
        total_time = time.time() - start_time
        progress_bar.progress(1.0)
        status_text.text(f"‚úÖ Exact matching completed! Found {len(matches):,} matches in {total_time:.1f}s")

        return pd.DataFrame(matches)


    def _construct_original_full_address(self, row, registry_type):
        """Construct original full address from non-normalized components"""
        street = str(row.get("STREET_NAME", "")).strip()
        sub_street = str(row.get("SUB_STREET_NAME", "")).strip() if registry_type == "CAD" else ""
        house = str(row.get("HOUSE", "")).strip()
        building = str(row.get("BUILDING", "")).strip()
        
        # For CAD registry, we can also include additional address components if needed
        if registry_type == "CAD":
            # Get the original ADDRESS field if it exists (it contains the full formatted address)
            original_address = row.get("ADDRESS", "")
            if original_address:
                return str(original_address).strip()
        
        # Construct from components (include sub_street for Cadastre)
        if registry_type == "CAD":
            components = [comp for comp in [street, sub_street, house, building] if comp]
        else:
            components = [comp for comp in [street, house, building] if comp]
        return " ".join(components)

    def _create_match_record(self, spr_row, cad_row, score, match_type, candidates_count=1):
        """Create standardized match record"""
        return {
            "ADDRESS_ID_SPR": spr_row.get("ADDRESS_ID", ""),
            "STREET_NAME_SPR": spr_row.get("STREET_NAME", ""),
            "HOUSE_SPR": spr_row.get("HOUSE", ""),
            "BUILDING_SPR": spr_row.get("BUILDING", ""),
            "FULL_ADDRESS_SPR": spr_row.get("FULL_ADDRESS", ""),
            "ADDRESS_ID_CAD": cad_row.get("ADDRESS_ID", ""),
            "STREET_NAME_CAD": cad_row.get("STREET_NAME", ""),
            "SUB_STREET_NAME_CAD": cad_row.get("SUB_STREET_NAME", ""),
            "HOUSE_CAD": cad_row.get("HOUSE", ""),
            "BUILDING_CAD": cad_row.get("BUILDING", ""),
            "FULL_ADDRESS_CAD": cad_row.get("FULL_ADDRESS", ""),
            # Original street names (before normalization)
            "ORIGINAL_STREET_NAME_SPR": spr_row.get("STREET_NAME", ""),
            "ORIGINAL_STREET_NAME_CAD": cad_row.get("STREET_NAME", ""),
            "ORIGINAL_SUB_STREET_NAME_CAD": cad_row.get("SUB_STREET_NAME", ""),
            # Original full addresses (constructed from original components)
            "ORIGINAL_FULL_ADDRESS_SPR": self._construct_original_full_address(spr_row, "SPR"),
            "ORIGINAL_FULL_ADDRESS_CAD": self._construct_original_full_address(cad_row, "CAD"),
            # Normalized full addresses (these are already normalized)
            "NORMALIZED_FULL_ADDRESS_SPR": spr_row.get("FULL_ADDRESS", ""),
            "NORMALIZED_FULL_ADDRESS_CAD": cad_row.get("FULL_ADDRESS", ""),
            "MATCH_SCORE": score,
            "MATCH_TYPE": match_type,
            "CANDIDATES_COUNT": candidates_count,
            "COMPLETENESS_SPR": spr_row.get("COMPLETENESS_SCORE", 0),
            "COMPLETENESS_CAD": cad_row.get("COMPLETENESS_SCORE", 0),
            "MATCH_TIMESTAMP": datetime.now().isoformat()
        }




def load_registry_data_from_csv(source: str, file_path: str) -> pd.DataFrame:
    try:
        df = pd.read_csv(file_path)
        st.success(f"{source} data loaded successfully from CSV")
        return df
    except Exception as e:
        st.error(f"Failed to load {source} data: {e}")
        return pd.DataFrame()


# @st.cache_data
def preprocess_registries(_spr_df, _cad_df):
    """Comprehensive data preprocessing pipeline"""
    normalizer = AddressNormalizer()

    def process_registry(df, registry_name):
        """Process individual registry with comprehensive normalization"""
        processed = df.copy()

        # Handle missing values
        processed['STREET_NAME'] = processed['STREET_NAME'].fillna('')
        processed['HOUSE'] = processed['HOUSE'].fillna('')
        processed['BUILDING'] = processed['BUILDING'].fillna('')
        
        # Handle SUB_STREET_NAME for Cadastre registry
        if 'SUB_STREET_NAME' in processed.columns:
            processed['SUB_STREET_NAME'] = processed['SUB_STREET_NAME'].fillna('')
        else:
            processed['SUB_STREET_NAME'] = ''

        # Normalize fields
        processed['STREET_NORM'] = processed['STREET_NAME'].apply(normalizer.normalize)
        processed['SUB_STREET_NORM'] = processed['SUB_STREET_NAME'].apply(normalizer.normalize)
        processed['HOUSE_NORM'] = processed['HOUSE'].apply(normalizer.normalize_number_part)
        processed['BUILDING_NORM'] = processed['BUILDING'].apply(normalizer.normalize_number_part)

        # Create composite addresses (STREET + SUB_STREET + HOUSE + BUILDING)
        processed['FULL_ADDRESS'] = (
                processed['STREET_NORM'] + " " +
                processed['SUB_STREET_NORM'] + " " +
                processed['HOUSE_NORM'] + " " +
                processed['BUILDING_NORM']
        ).str.strip()
        
        # Clean up multiple spaces
        processed['FULL_ADDRESS'] = processed['FULL_ADDRESS'].str.replace(r'\s+', ' ', regex=True).str.strip()

        # Create search keys
        processed['SEARCH_KEY'] = (
                processed['STREET_NORM'] + "_" + processed['HOUSE_NORM']
        )

        # Data quality metrics (include SUB_STREET_NAME for Cadastre)
        if 'SUB_STREET_NAME' in df.columns and registry_name == "Cadastre":
            # For Cadastre: STREET_NAME + SUB_STREET_NAME + HOUSE + BUILDING
            processed['COMPLETENESS_SCORE'] = (
                processed['STREET_NAME'].notna().astype(int) +
                processed['SUB_STREET_NAME'].notna().astype(int) +
                processed['HOUSE'].notna().astype(int) +
                processed['BUILDING'].notna().astype(int)
            ) / 4
        else:
            # For SPR: STREET_NAME + HOUSE + BUILDING
            processed['COMPLETENESS_SCORE'] = (
                processed['STREET_NAME'].notna().astype(int) +
                processed['HOUSE'].notna().astype(int) +
                processed['BUILDING'].notna().astype(int)
            ) / 3

        return processed

    spr_processed = process_registry(_spr_df, "SPR")
    cad_processed = process_registry(_cad_df, "Cadastre")

    return spr_processed, cad_processed


def analyze_data_quality(df, registry_name):
    """Comprehensive data quality analysis"""
    quality_metrics = {
        'total_records': len(df),
        'street_completeness': df['STREET_NAME'].notna().sum() / len(df),
        'house_completeness': df['HOUSE'].notna().sum() / len(df),
        'building_completeness': df['BUILDING'].notna().sum() / len(df),
        'unique_streets': df['STREET_NORM'].nunique(),
        'avg_completeness': df['COMPLETENESS_SCORE'].mean(),
        'duplicate_addresses': len(df) - df['FULL_ADDRESS'].nunique(),
    }

    return quality_metrics


def get_unmatched_street_names(spr_processed, cad_processed, matched_df=None):
    """Get unmatched street names between SPR and Cadastre registries"""
    
    # Get unique street names from both registries
    spr_streets = set(spr_processed['STREET_NORM'].dropna().unique())
    cad_streets = set(cad_processed['STREET_NORM'].dropna().unique())
    
    # Remove empty strings
    spr_streets.discard('')
    cad_streets.discard('')
    
    # Find streets in SPR but missing in Cadastre
    spr_only_streets = spr_streets - cad_streets
    
    # Find streets in Cadastre but missing in SPR
    cad_only_streets = cad_streets - spr_streets
    
    # Create dataframes for display
    spr_missing_df = pd.DataFrame({
        'STREET_NAME': sorted(list(spr_only_streets))
    }) if spr_only_streets else pd.DataFrame(columns=['STREET_NAME'])
    
    cad_missing_df = pd.DataFrame({
        'STREET_NAME': sorted(list(cad_only_streets))
    }) if cad_only_streets else pd.DataFrame(columns=['STREET_NAME'])
    
    return spr_missing_df, cad_missing_df