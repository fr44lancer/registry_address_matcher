import pandas as pd
import numpy as np
from collections import defaultdict, Counter
import re
import logging
from datetime import datetime
from sqlalchemy import create_engine, text
# Removed rapidfuzz import - not needed for exact matching only
import streamlit as st
import time




class AddressNormalizer:
    def __init__(self):
        self.aliases = {
            "‘Ω. ’Ä‘±’Ö’ê‘ª‘ø": "‘Ω’ê‘ª’Ñ’Ö‘±’Ü ’Ä‘±’Ö’ê‘ª‘ø‘ª",
            "‘Ω’ê‘ª’Ñ’Ö‘±’Ü ’Ä‘±’Ö’ê‘ª‘ø": "‘Ω’ê‘ª’Ñ’Ö‘±’Ü ’Ä‘±’Ö’ê‘ª‘ø‘ª",
        }

        self.armenian_suffixes = [
            r'\b‘Ω’É’Ç\.?',
            r'\b‘Ω’É\.?',
            r'\b÷Ä’§\.?',
            r'\b’´’∂\.?',
            r'\b’Ü’ê‘≤\.?',
            r'\b‘≤’Ü\.?',
            r'\b’É’Ç\.?',
            r'\b’ì\.?',
            r'\b’ä’à’Ç\.?',
            r'\b‘±’é\.?',
            r'\b’É‘±’Ñ‘≤\.?',
            r'\b’è’à’í’Ü\.?',
            r'\b’è‚Ä§\.?',
            r'\b\.?',
            r'\b’Ä’∏’≤\.?',
            r'\b’ì‘±‘ø\.?',
            r'\b’∑’°÷Ä÷Ñ\.?',
            r'\b‘π‘µ‘º‘±\.?'
        ]

        # Normalize old and new keys in this mapping
        self.old_to_new_map = {
            self._norm("’ñ÷Ä’∏÷Ç’∂’¶’•’´"): self._norm("‘º. ’Ñ’°’§’∏’µ’°’∂"),
            self._norm("‘º’•’∂’´’∂’£÷Ä’°’§’µ’°’∂"): self._norm("’é. ’ç’°÷Ä’£’Ω’µ’°’∂"),
            self._norm("‘ø’´÷Ä’∏’æ’°’Ø’°’∂’µ’°’∂"): self._norm("’é’°’∂’°’±’∏÷Ä’´"),
            self._norm("‘ø’°’¨’´’∂’´’∂’´"): self._norm("‘≥. ’Ü’™’§’•’∞’´"),
            self._norm("‘ø’´’∂’£’´’Ω’•’∫’´"): self._norm("’é. ’â’•÷Ä’°’¶’´"),
            self._norm("’ä’¨’•’≠’°’∂’∏’æ’´"): self._norm("’ç’°’∞’¥’°’∂’°’∫’°’∞’∂’•÷Ä’´"),
            self._norm("’á’´’∂’°÷Ä’°÷Ä’∂’•÷Ä’´"): self._norm("’Ñ. ‘π’•’ø’π’•÷Ä’´"),
            self._norm("‘ø’´÷Ä’∏’æ’´"): self._norm("’Ü. ’å’´’™’Ø’∏’æ’´"),
            self._norm("‘º’•’∂’´’∂’´"): self._norm("’è’´’£÷Ä’°’∂ ’Ñ’•’Æ’´"),
            self._norm("‘Ω. ’Ä’°’µ÷Ä’´’Ø"): self._norm("‘Ω÷Ä’´’¥’µ’°’∂ ’Ä’°’µ÷Ä’´’Ø’´"),
            self._norm("‘±’∂’´ ’©’°’≤’°’¥’°’Ω ’Ñ. ‘±’æ’•’ø’´’Ω’µ’°’∂"): self._norm("’Ñ. ‘±’æ’•’ø’´’Ω’µ’°’∂"),
            self._norm("’Ñ’°÷Ä÷Ñ’Ω’´"): self._norm("’ä. ’ã’°÷É’°÷Ä’´’±’•’´"),
            self._norm("‘±’∂’´ ’©’°’≤’°’¥’°’Ω ‘±. ’á’°’∞’´’∂’µ’°’∂"): self._norm("‘±. ’á’°’∞’´’∂’µ’°’∂"),
            self._norm("’ï’≤’°’Ø’°’µ’´’∂"): self._norm("‘±÷Ä÷á’•’¨’µ’°’∂ ’∑÷Ä’ª’°’∂÷Å’∏’≤"),
            self._norm("’å’•’∫’´’∂’´"): self._norm("‘≤. ’á’π’•÷Ä’¢’´’∂’°’µ’´"),
            self._norm("’Ä’•’≤’°÷É’∏’≠’∏÷Ç’©’µ’°’∂"): self._norm("‘≥. ’Ü’™’§’•’∞’´"),
            self._norm("‘±’∂’´ ’©’°’≤’°’¥’°’Ω ‘µ. ’â’°÷Ä’•’∂÷Å’´"): self._norm("‘µ. ’â’°÷Ä’•’∂÷Å’´"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 10-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 15-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 11-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 12-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 13-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å 14-÷Ä’§"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("’ç÷á’µ’°’∂"): self._norm("’Ä. ’Ç’°’∂’§’´’¨’µ’°’∂"),
            self._norm("’Ñ’∏÷Ç’∑-2  ’©’°’≤’°’¥’°’Ω’´ ÷É’∏’≤’∏÷Å’∂’•÷Ä’´÷Å ’¥’•’Ø’®"): self._norm("‘ø. ’Ä’°’¨’°’¢’µ’°’∂"),
            self._norm("’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂"): self._norm("’Ö. ’é’°÷Ä’§’°’∂’µ’°’∂"),
            self._norm("‘Ω’°’≤’°’≤’∏÷Ç’©’µ’°’∂"): self._norm("‘≤’°’£÷Ä’°’ø’∏÷Ç’∂’µ’°÷Å"),
            self._norm("’Ñ’°÷Ä÷Ñ’Ω’´"): self._norm("’ã’´’æ’°’∂’∏÷Ç"),
            self._norm("‘±’¶’´’¶’¢’•’Ø’∏’æ’´"): self._norm("’Ü. ’á’∂’∏÷Ä’∞’°’¨’∏÷Ç"),
            self._norm("‘∑’¨’•’Ø’ø÷Ä’∏ ’∫÷Ä’´’¢’∏÷Ä’∂’´ 6-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘±. ‘±÷Ä’¥’•’∂’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘∑’¨’•’Ø’ø÷Ä’∏ ’∫÷Ä’´’¢’∏÷Ä’∂’´ 10-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘±. ‘≥÷á’∏÷Ä’£’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘ø’´÷Ä’∏’æ’°’¢’°’§’µ’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("‘±. ‘π’°’¥’°’∂’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("50 ’°’¥’µ’°’Ø’´ ’°’∂’æ’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("‘±. ’Ñ’°’∂’∏÷Ç’Ø’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("<<‘±’∂’´>> ’©’°’≤’°’¥’°’Ω 3-÷Ä’§ ÷É’∏’≤’∏÷Å"): self._norm("‘±. ’á’°’∞’´’∂’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("’Ä’∂’∏÷Å’°’æ’°’∂ 2-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘±. ’ä’•’ø÷Ä’∏’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘ø’∏’¥’Ω’∏’¥’∏’¨’´ ÷É’∏’≤’∏÷Å"): self._norm("‘±. ’é’°’Ω’´’¨’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘ø’•÷Å’≠’∏’æ’•’¨’´ ÷É’∏’≤’∏÷Å"): self._norm("‘±÷Ä’ø’°’Ø ’•’∫’´’Ω’Ø’∏’∫’∏’Ω ’ç’¥’¢’°’ø’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘±÷Ä’æ’•’¨’°’±’• ÷É’∏’≤’∏÷Å"): self._norm("‘≥’°÷Ä’•’£’´’∂ ‘±-’´ ÷É’∏’≤’∏÷Å"),
            self._norm("‘∑’¨’•’Ø’ø÷Ä’∏ ’∫÷Ä’´’¢’∏÷Ä’∂’´ 8-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘π. ’Ñ’°’∂’§’°’¨’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("’ä’∏’≤’∫’°’ø’°’æ’°’∂ 3-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘∫. ‘≤. ‘≤’°÷Ä’∏’∂’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘ø÷Ä’∏÷Ç’∫’Ω’Ø’°’µ’° ÷É’∏’≤’∏÷Å"): self._norm("‘Ω. ‘¥’°’∑’ø’•’∂÷Å’´ ÷É’∏’≤’∏÷Å"),
            self._norm("’î’∏÷Ç’©’°’´’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("‘ø. ‘¥’•’¥’´÷Ä’≥’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("’ä’∏’≤’∫’°’ø’°’æ’°’∂ 2-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("‘ø. ‘Ω’°’π’°’ø÷Ä’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘ø’∏÷Ç’µ’¢’´’∑÷á’´ ÷É’∏’≤’∏÷Å"): self._norm("’Ä. ’Ñ’°’¶’¥’°’∂’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("’ä’´’∏’∂’•÷Ä’°’Ø’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("’Ä. ’Ñ’•’¨÷Ñ’∏’∂’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("’ä’∏’≤’∫’°’ø’°’æ’°’∂ 1-’´’∂ ’∑’°÷Ä÷Ñ"): self._norm("’Ä. ’ä’∏’≤’∏’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("’ä’∏’≤’∫’°’ø’°’æ’°’∂ 4-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("’Ä. ’å’°’Ω’Ø’°’ø’¨’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘ø’°’ø’•’¨’∂’°’µ’°"): self._norm("’Ä’∂’∏÷Å’°’æ’°’∂’´ 1-’´’∂ ’∑’°÷Ä÷Ñ"),
            self._norm("’ä’•’ø ’¢’°÷Ä’°’Ø’∂’•÷Ä"): self._norm("’Ç. ’Ç’∏÷Ç’Ø’°’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("’Ñ’°’µ’´’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("’Ñ. ’Ñ’Ø÷Ä’ø’π’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘∑’¨’•’Ø’ø÷Ä’∏ ’∫÷Ä’´’¢’∏÷Ä’∂’´ 7-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("’Ñ. ’ç’°÷Ä’£’Ω’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("’ç’æ’•÷Ä’§’¨’∏’æ’´ ÷É’∏’≤’∏÷Å"): self._norm("’Ü. ’Ç’∏÷Ä’≤’°’∂’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘±’Ω’ø’≤’´ ’∞÷Ä’°’∫’°÷Ä’°’Ø"): self._norm("’á. ‘±’¶’∂’°’æ’∏÷Ç÷Ä’´ ’∞÷Ä’°’∫’°÷Ä’°’Ø"),
            self._norm("’ç. ’Ñ’∏÷Ç’Ω’°’µ’•’¨’µ’°’∂ ÷É’∏’≤’∏÷Å"): self._norm("’á. ‘±’¶’∂’°’æ’∏÷Ç÷Ä’´ ’∞÷Ä’°’∫’°÷Ä’°’Ø"),
            self._norm("‘∑’¨’•’Ø’ø÷Ä’∏ ’∫÷Ä’´’¢’∏÷Ä’∂’´ 11-÷Ä’§ ’∑’°÷Ä÷Ñ"): self._norm("’å. ‘¥’°’∂’´’•’¨’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("’ï÷Ä’ª’∏’∂’´’Ø’´’±’•’´ ÷É’∏’≤’∏÷Å"): self._norm("’ç. ’Ñ’°’ø’∂’´’∑’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘∑’∂’£’•’¨’Ω’´ ÷É’∏’≤’∏÷Å"): self._norm("’é. ‘±’≥’•’¥’µ’°’∂ ÷É’∏’≤’∏÷Å"),
            self._norm("‘ø’•’∂’ø÷Ä’∏’∂’°’Ø’°’∂ ’∞÷Ä’°’∫’°÷Ä’°’Ø"): self._norm("’é’°÷Ä’§’°’∂’°’∂÷Å ’∞÷Ä’°’∫’°÷Ä’°’Ø"),
            self._norm("<<‘±’∂’´>> ’©’°’≤’°’¥’°’Ω 15-÷Ä’§ ÷É’∏’≤’∏÷Å"): self._norm("’ñ’∏÷Ä’°’¨’¢’•÷Ä’£’´ ÷É’∏’≤’∏÷Å"),
        }

    def _norm(self, text):
        text = str(text).strip().upper()
        text = re.sub(r'[^\w\s]', '', text)
        return re.sub(r'\s+', ' ', text)

    def normalize(self, text):
        if pd.isna(text):
            return ""

        text = str(text).strip().upper()

        # Apply direct alias replacement first
        if text in self.aliases:
            text = self.aliases[text]

        # Remove suffixes
        for suffix in self.armenian_suffixes:
            text = re.sub(suffix, '', text, flags=re.IGNORECASE)

        # Remove special chars, extra spaces
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\s+', ' ', text)

        # Remove trailing "‘ª" if it's the last letter of each word
        text = ' '.join([w[:-1] if w.endswith("‘ª") else w for w in text.split()])

        # Normalize again and map old names to new ones
        text_norm = self._norm(text)
        return self.old_to_new_map.get(text_norm, text_norm)


class AdvancedAddressMatcher:
    def __init__(self, spr_df, cad_df, max_records=None):
        # Limit SPR records if specified
        if max_records is not None and max_records < len(spr_df):
            self.spr_df = spr_df.head(max_records).copy()
            st.info(f"üìä Processing limited to {max_records:,} SPR records out of {len(spr_df):,} total records")
        else:
            self.spr_df = spr_df.copy()

        self.cad_df = cad_df
        self.original_spr_count = len(spr_df)
        self.processing_count = len(self.spr_df)

        # Create indices based on the limited dataset
        self.street_index = self._create_street_index()
        self.house_index = self._create_house_index()

    def _create_street_index(self):
        """Create street-based index for fast lookups"""
        index = defaultdict(list)
        for idx, row in self.cad_df.iterrows():
            street = row['STREET_NORM']
            if street:
                index[street].append(idx)
        return dict(index)

    def _create_house_index(self):
        """Create house-based index for fast lookups"""
        index = defaultdict(list)
        for idx, row in self.cad_df.iterrows():
            house = row['HOUSE_NORM']
            if house:
                index[house].append(idx)
        return dict(index)


    def find_exact_matches(self):
        """Find exact matches using full address matching with detailed progress"""
        matches = []
        total_spr = len(self.spr_df)

        # Create progress tracking components
        progress_container = st.container()
        with progress_container:
            st.subheader("üéØ Exact Matching Progress")
            progress_bar = st.progress(0)
            status_text = st.empty()
            stats_cols = st.columns(4)

            with stats_cols[0]:
                processed_metric = st.empty()
            with stats_cols[1]:
                matches_metric = st.empty()
            with stats_cols[2]:
                rate_metric = st.empty()
            with stats_cols[3]:
                eta_metric = st.empty()

        start_time = time.time()

        # Full address exact match
        status_text.text("üîç Finding exact address matches...")
        cad_full_lookup = {row['FULL_ADDRESS']: idx for idx, row in self.cad_df.iterrows()}

        chunk_size = 500

        for i in range(0, total_spr, chunk_size):
            if st.session_state.get('stop_requested', False):
                break

            chunk = self.spr_df.iloc[i:i + chunk_size]
            chunk_matches = 0

            for idx, spr_row in chunk.iterrows():
                full_address = spr_row['FULL_ADDRESS']
                if full_address in cad_full_lookup:
                    cad_idx = cad_full_lookup[full_address]
                    matches.append(self._create_match_record(spr_row, self.cad_df.iloc[cad_idx], 100, "EXACT_FULL"))
                    chunk_matches += 1

            # Update progress
            processed = min(i + chunk_size, total_spr)
            progress = processed / total_spr
            elapsed_time = time.time() - start_time

            # Calculate ETA
            if processed > 0:
                rate = processed / elapsed_time
                eta = (total_spr - processed) / rate if rate > 0 else 0
            else:
                eta = 0

            # Update UI
            progress_bar.progress(progress)
            status_text.text(
                f"Processed {processed:,}/{total_spr:,} records | Found {chunk_matches} matches in current chunk")

            processed_metric.metric("Processed", f"{processed:,}/{total_spr:,}")
            matches_metric.metric("Exact Matches", f"{len(matches):,}")
            rate_metric.metric("Rate", f"{rate:.1f} records/sec" if rate > 0 else "Calculating...")
            eta_metric.metric("ETA", f"{eta:.1f}s" if eta > 0 else "Calculating...")

        # Final update
        total_time = time.time() - start_time
        progress_bar.progress(1.0)
        status_text.text(f"‚úÖ Exact matching completed! Found {len(matches):,} matches in {total_time:.1f}s")

        return pd.DataFrame(matches)


    def _create_match_record(self, spr_row, cad_row, score, match_type, candidates_count=1):
        """Create standardized match record"""
        return {
            "ADDRESS_ID_SPR": spr_row.get("ADDRESS_ID", ""),
            "STREET_NAME_SPR": spr_row.get("STREET_NAME", ""),
            "HOUSE_SPR": spr_row.get("HOUSE", ""),
            "BUILDING_SPR": spr_row.get("BUILDING", ""),
            "FULL_ADDRESS_SPR": spr_row.get("FULL_ADDRESS", ""),
            "ADDRESS_ID_CAD": cad_row.get("ADDRESS_ID", ""),
            "STREET_NAME_CAD": cad_row.get("STREET_NAME", ""),
            "HOUSE_CAD": cad_row.get("HOUSE", ""),
            "BUILDING_CAD": cad_row.get("BUILDING", ""),
            "FULL_ADDRESS_CAD": cad_row.get("FULL_ADDRESS", ""),
            "MATCH_SCORE": score,
            "MATCH_TYPE": match_type,
            "CANDIDATES_COUNT": candidates_count,
            "COMPLETENESS_SPR": spr_row.get("COMPLETENESS_SCORE", 0),
            "COMPLETENESS_CAD": cad_row.get("COMPLETENESS_SCORE", 0),
            "MATCH_TIMESTAMP": datetime.now().isoformat()
        }




def load_registry_data_from_csv(source: str, file_path: str) -> pd.DataFrame:
    try:
        df = pd.read_csv(file_path)
        st.success(f"{source} data loaded successfully from CSV")
        return df
    except Exception as e:
        st.error(f"Failed to load {source} data: {e}")
        return pd.DataFrame()


@st.cache_data
def preprocess_registries(_spr_df, _cad_df):
    """Comprehensive data preprocessing pipeline"""
    normalizer = AddressNormalizer()

    def process_registry(df, registry_name):
        """Process individual registry with comprehensive normalization"""
        processed = df.copy()

        # Handle missing values
        processed['STREET_NAME'] = processed['STREET_NAME'].fillna('')
        processed['HOUSE'] = processed['HOUSE'].fillna('')
        processed['BUILDING'] = processed['BUILDING'].fillna('')

        # Normalize fields
        processed['STREET_NORM'] = processed['STREET_NAME'].apply(normalizer.normalize)
        processed['HOUSE_NORM'] = processed['HOUSE'].apply(normalizer.normalize)
        processed['BUILDING_NORM'] = processed['BUILDING'].apply(normalizer.normalize)

        # Create composite addresses
        processed['FULL_ADDRESS'] = (
                processed['STREET_NORM'] + " " +
                processed['HOUSE_NORM'] + " " +
                processed['BUILDING_NORM']
        ).str.strip()

        # Create search keys
        processed['SEARCH_KEY'] = (
                processed['STREET_NORM'] + "_" + processed['HOUSE_NORM']
        )

        # Data quality metrics
        processed['COMPLETENESS_SCORE'] = (
                                                  processed['STREET_NAME'].notna().astype(int) +
                                                  processed['HOUSE'].notna().astype(int) +
                                                  processed['BUILDING'].notna().astype(int)
                                          ) / 3

        return processed

    spr_processed = process_registry(_spr_df, "SPR")
    cad_processed = process_registry(_cad_df, "Cadastre")

    return spr_processed, cad_processed


def analyze_data_quality(df, registry_name):
    """Comprehensive data quality analysis"""
    quality_metrics = {
        'total_records': len(df),
        'street_completeness': df['STREET_NAME'].notna().sum() / len(df),
        'house_completeness': df['HOUSE'].notna().sum() / len(df),
        'building_completeness': df['BUILDING'].notna().sum() / len(df),
        'unique_streets': df['STREET_NORM'].nunique(),
        'avg_completeness': df['COMPLETENESS_SCORE'].mean(),
        'duplicate_addresses': len(df) - df['FULL_ADDRESS'].nunique(),
    }

    return quality_metrics